{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ST-MTL_Segmentation_Demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c55d85f2cff14860aa852e226b891f11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_36e45aa816db4832b5255a5e94fea252",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c4c84d2347d44faa8a3344509bda6cfa",
              "IPY_MODEL_cc3dd8c2fb9548f78204204f7af359b3"
            ]
          }
        },
        "36e45aa816db4832b5255a5e94fea252": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c4c84d2347d44faa8a3344509bda6cfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b149fc63bf1444409b0c9d0f2ee49cdd",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 46827520,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 46827520,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_725f113e68924190af906a15a7924342"
          }
        },
        "cc3dd8c2fb9548f78204204f7af359b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_72c1e93e3a494a92a6980c944e8d2186",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 74.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b7e36ae1341446b880a56dc9ae3896bd"
          }
        },
        "b149fc63bf1444409b0c9d0f2ee49cdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "725f113e68924190af906a15a7924342": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "72c1e93e3a494a92a6980c944e8d2186": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b7e36ae1341446b880a56dc9ae3896bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/ST-MTL/blob/main/ST_MTL_Segmentation_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUJMdlmnIDSP"
      },
      "source": [
        "# ST-MTL: Spatio-Temporal multitask learning model to predict scanpath while tracking instruments in robotic surgery\n",
        "\n",
        "Representation learning of the task-oriented attention while tracking instrument holds vast potential in image-guided robotic surgery. Incorporating cognitive ability to automate the camera control enables the surgeon to concentrate more on dealing with surgical instruments. The objective is to reduce the operation time and facilitate the surgery for both surgeons and patients. We propose an end-to-end trainable Spatio-Temporal Multi-Task Learning (ST-MTL) model with a shared encoder and spatio-temporal decoders for the real-time surgical instrument segmentation and task-oriented saliency detection. In the MTL model of shared-parameters, optimizing multiple loss functions into a convergence point is still an open challenge. We tackle the problem with a novel asynchronous spatio-temporal optimization (ASTO) technique by calculating independent gradients for each decoder. We also design a competitive squeeze and excitation unit by casting a skip connection that retains weak features, excites strong features, and performs dynamic spatial and channel-wise feature recalibration. To capture better long term spatio-temporal dependencies, we enhance the long-short term memory (LSTM) module by concatenating high-level encoder features of consecutive frames. We also introduce Sinkhorn regularized loss to enhance task-oriented saliency detection by preserving computational efficiency. We generate the task-aware saliency maps and scanpath of the instruments on the dataset of the MICCAI 2017 robotic instrument segmentation challenge. Compared to the state-of-the-art segmentation and saliency methods, our model outperforms most of the evaluation metrics and produces an outstanding performance in the challenge.\n",
        "\n",
        "Paper: [ST-MTL: Spatio-Temporal multitask learning model to predict scanpath while tracking instruments in robotic surgery](https://www.sciencedirect.com/science/article/pii/S1361841520302012) <br>\n",
        "Code Architecture only: https://github.com/mobarakol/ST-MTL<br>\n",
        "\n",
        "Instrument Classes:\"Bipolar Forceps\": 1, \"Prograsp Forceps\": 2, \"Large Needle Driver\": 3, \"Vessel Sealer\": 4, \"Grasping Retractor\": 5, \"Monopolar Curve, Scissors\": 6, \"Other\": 7<br>\n",
        "\n",
        "\n",
        "## Citation\n",
        "If you use this code for your research, please cite our paper.\n",
        "\n",
        "```\n",
        "@article{islam2021st,\n",
        "  title={ST-MTL: Spatio-Temporal multitask learning model to predict scanpath while tracking instruments in robotic surgery},\n",
        "  author={Islam, Mobarakol and Vibashan, VS and Lim, Chwee Ming and Ren, Hongliang},\n",
        "  journal={Medical Image Analysis},\n",
        "  volume={67},\n",
        "  pages={101837},\n",
        "  year={2021},\n",
        "  publisher={Elsevier}\n",
        "}\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diuXaiY0Jygj"
      },
      "source": [
        "Download Code, Data and Trained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpBPJJFxKjdU"
      },
      "source": [
        "Download Code from Github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsSeawSLKAQl",
        "outputId": "4e380d2b-f283-4d25-f721-f1c38c92803a"
      },
      "source": [
        "!rm -rf ST-MTL\n",
        "!git clone https://github.com/mobarakol/ST-MTL.git\n",
        "%cd ST-MTL"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ST-MTL'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 15 (delta 1), reused 9 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (15/15), done.\n",
            "/content/ST-MTL\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_YO46L3K-Jh"
      },
      "source": [
        "Download Validation Data and Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7Igmt98H-cc"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_rKIlv3K04W"
      },
      "source": [
        "ids = ['1UqoSVJLpF6W9F5PfCitDibieZPbd4lJh', '1rx0oVv8eDoK3bXNT362Kvl6b1y4knxb-']\n",
        "zip_files = ['Instrument_17.zip','best_epoch_st-mtl.pth.tar']\n",
        "for id, zip_file in zip(ids, zip_files):\n",
        "    downloaded = drive.CreateFile({'id':id}) \n",
        "    downloaded.GetContentFile(zip_file)\n",
        "    if zip_file[-3:] == 'zip':\n",
        "        !unzip -q $zip_file"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tjk7whIFL5fB"
      },
      "source": [
        "## Demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137,
          "referenced_widgets": [
            "c55d85f2cff14860aa852e226b891f11",
            "36e45aa816db4832b5255a5e94fea252",
            "c4c84d2347d44faa8a3344509bda6cfa",
            "cc3dd8c2fb9548f78204204f7af359b3",
            "b149fc63bf1444409b0c9d0f2ee49cdd",
            "725f113e68924190af906a15a7924342",
            "72c1e93e3a494a92a6980c944e8d2186",
            "b7e36ae1341446b880a56dc9ae3896bd"
          ]
        },
        "id": "sXX5AMfKLc-2",
        "outputId": "07b7c2c2-daad-4824-b020-e8549fca4f74"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import argparse\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from model import ST_MTL_SEG\n",
        "from dataset import SurgicalDataset\n",
        "from utils import seed_everything, calculate_dice, calculate_confusion_matrix_from_arrays\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def validate(valid_loader, model, args):\n",
        "    confusion_matrix = np.zeros(\n",
        "            (args.num_classes, args.num_classes), dtype=np.uint32)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, labels_seg, _,_) in enumerate(valid_loader):\n",
        "            inputs, labels_seg = inputs.to(device), np.array(labels_seg)\n",
        "            pred_seg = model(inputs)\n",
        "            pred_seg = pred_seg.data.max(1)[1].squeeze_(1).cpu().numpy()\n",
        "            confusion_matrix += calculate_confusion_matrix_from_arrays(\n",
        "                pred_seg, labels_seg, args.num_classes)    \n",
        "\n",
        "    confusion_matrix = confusion_matrix[1:, 1:]  # exclude background\n",
        "    dices = {'dice_{}'.format(cls + 1): dice\n",
        "                for cls, dice in enumerate(calculate_dice(confusion_matrix))}\n",
        "    dices_per_class = np.array(list(dices.values()))          \n",
        "\n",
        "    return dices_per_class\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description='Instrument Segmentation')\n",
        "    parser.add_argument('--num_classes', default=8, type=int, help=\"num of classes\")\n",
        "    parser.add_argument('--data_root', default='Instrument_17', help=\"data root dir\")\n",
        "    parser.add_argument('--batch_size', default=2, type=int, help=\"num of classes\")\n",
        "    args = parser.parse_args(args=[])\n",
        "    dataset_test = SurgicalDataset(data_root=args.data_root, seq_set=[4,7], is_train=False)\n",
        "    test_loader = DataLoader(dataset=dataset_test, batch_size=args.batch_size, shuffle=False, num_workers=2,\n",
        "                              drop_last=True)\n",
        "    \n",
        "    print('Sample size of test dataset:', dataset_test.__len__())\n",
        "    model = ST_MTL_SEG(num_classes=args.num_classes).to(device)\n",
        "    model.load_state_dict(torch.load('best_epoch_st-mtl.pth.tar'))\n",
        "    model.eval()\n",
        "    dices_per_class = validate(test_loader, model, args)\n",
        "    print('Mean Avg Dice:%.4f [Bipolar Forceps:%.4f, Prograsp Forceps:%.4f, Large Needle Driver:%.4f, Vessel Sealer:%.4f]'\n",
        "        %(dices_per_class[:4].mean(),dices_per_class[0], dices_per_class[1],dices_per_class[2],dices_per_class[3]))\n",
        "    \n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    class_names = [\"Bipolar Forceps\", \"Prograsp Forceps\", \"Large Needle Driver\", \"Vessel Sealer\", \"Grasping Retractor\", \"Monopolar Curve, Scissors\", \"Other\"]\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    seed_everything()\n",
        "    main()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample size of test dataset: 448\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c55d85f2cff14860aa852e226b891f11",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Mean Avg Dice:0.7560 [Bipolar Forceps:0.7344, Prograsp Forceps:0.6825, Large Needle Driver:0.7814, Vessel Sealer:0.8255]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}